--- git status ---
HEAD detached from dc9bbf2
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   deploy/config.py
	modified:   source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py
	modified:   source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/deploy/config.py b/deploy/config.py
index 88989ea..13c325f 100644
--- a/deploy/config.py
+++ b/deploy/config.py
@@ -7,7 +7,7 @@ sys.path.append(dir_path+"/../scripts/rsl_rl")
 robot = 'aliengo'  # 'aliengo', 'go1', 'go2', 'b2', 'hyqreal1', 'hyqreal2', 'mini_cheetah'  # TODO: Load from robot_descriptions.py
 
 #policy_path = ".."
-policy_path = dir_path + "/../tested_policies/" + robot + "/8k_128_128_128_stop" + "/exported/policy.onnx"
+policy_path = "/home/alienware/isaaclab_ws_home/basic-locomotion-dls-isaaclab/logs/rsl_rl/flat_direct/2025-07-22_22-52-20_8k_128_128_128_aliengo/exported/policy.onnx"
 
 # ----------------------------------------------------------------------------------------------------------------
 if(robot == "aliengo"):
diff --git a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py
index 0da22b7..0722310 100644
--- a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py
+++ b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py
@@ -45,7 +45,7 @@ class MorphologycalSymmetriesCfg:
 @configclass
 class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
     num_steps_per_env = 24
-    max_iterations = 500
+    max_iterations = 1500
     save_interval = 50
     experiment_name = "flat_direct"
     empirical_normalization = False
@@ -57,7 +57,7 @@ class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
         activation="elu",
     )
     algorithm = RslRlPpoAlgorithmCfg(
-        class_name="PPO", #PPO, PPOSymmDataAugmented #AMP_PPO
+        class_name="PPOSymmDataAugmented", #PPO, PPOSymmDataAugmented #AMP_PPO
         value_loss_coef=1.0,
         use_clipped_value_loss=True,
         clip_param=0.2,
@@ -86,8 +86,9 @@ class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
     )
 
     # Symmetry Related Stuff
+    history_length = 5
     morphologycal_symmetries_cfg = MorphologycalSymmetriesCfg(
-        obs_space_names = [
+        obs_space_names = ([
             "base_lin_vel:base",
             "base_ang_vel:base",
             "gravity:base",
@@ -96,7 +97,7 @@ class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
             "qvel_js",
             "actions",
             "clock_data",
-        ],
+        ])*int(history_length),
         
         action_space_names = ["actions"],
         
@@ -106,8 +107,6 @@ class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
             "FL_calf_joint", "FR_calf_joint", "RL_calf_joint", "RR_calf_joint"
         ],
         
-        history_length = 5,
-        
         robot_name = "a1",
     )
 
@@ -129,7 +128,7 @@ class RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
         activation="elu",
     )
     algorithm = RslRlPpoAlgorithmCfg(
-        class_name="PPO", #PPO, PPOSymmDataAugmented #AMP_PPO
+        class_name="PPOSymmDataAugmented", #PPO, PPOSymmDataAugmented #AMP_PPO
         value_loss_coef=1.0,
         use_clipped_value_loss=True,
         clip_param=0.2,
diff --git a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py
index e37d825..37c1164 100644
--- a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py
+++ b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py
@@ -69,7 +69,7 @@ class LocomotionEnv(DirectRLEnv):
         elif(cfg.desired_gait == "multigait"):
             #TODO: implement multigait
             raise NotImplementedError("Multigait not implemented yet")
-        self._phase_signal = self._phase_offset# + self.step_dt * self._step_freq * torch.rand(self.num_envs, 1, device=self.device)*10.
+        self._phase_signal = self._phase_offset.clone()# + self.step_dt * self._step_freq * torch.rand(self.num_envs, 1, device=self.device)*10.
         self._phase_signal = self._phase_signal % 1.0
 
 
@@ -181,25 +181,28 @@ class LocomotionEnv(DirectRLEnv):
         self._commands[:, :3] = self._commands[:, :3] * ~resample_time.unsqueeze(1).expand(-1, 3) + commands_resample * resample_time.unsqueeze(1).expand(-1, 3)
 
 
-        # Stop and Go
+        # Stop
+        rest_time = self.episode_length_buf >= self.max_episode_length - 50
+        self._commands[:, :3] *= ~rest_time.unsqueeze(1).expand(-1, 3)        
+
+
+        """# Stop and Go
+        rest_time = (self.episode_length_buf >= self.max_episode_length - 150) & (self.episode_length_buf <= self.max_episode_length - 100)
+        self._commands[:, :3] *= ~rest_time.unsqueeze(1).expand(-1, 3)        
+
         restart_time = self.episode_length_buf == self.max_episode_length - 99
         commands_restart = torch.zeros_like(self._commands).uniform_(-1.0, 1.0)
         commands_restart[:, 0] *= 0.5 * self._velocity_gait_multiplier
         commands_restart[:, 1] *= 0.25 
         commands_restart[:, 2] *= 0.3 
-        self._commands[:, :3] = self._commands[:, :3] * ~restart_time.unsqueeze(1).expand(-1, 3) + commands_restart * restart_time.unsqueeze(1).expand(-1, 3)
-
-        rest_time = (self.episode_length_buf >= self.max_episode_length - 150) & (self.episode_length_buf <= self.max_episode_length - 100)
-        self._commands[:, :3] *= ~rest_time.unsqueeze(1).expand(-1, 3)
+        self._commands[:, :3] = self._commands[:, :3] * ~restart_time.unsqueeze(1).expand(-1, 3) + commands_restart * restart_time.unsqueeze(1).expand(-1, 3)"""
 
 
         # Took some envs, and put to zero the vel
         if self.num_envs > 100:
             num_fixed_envs = 100
             fixed_env_ids = torch.arange(num_fixed_envs, device=self.device)
-            self._commands[fixed_env_ids, 0] = 0.0
-            self._commands[fixed_env_ids, 1] = 0.0
-            self._commands[fixed_env_ids, 2] = 0.0
+            self._commands[fixed_env_ids, :3] *= 0.0
 
 
         clock_data = None
@@ -749,7 +752,7 @@ class LocomotionEnv(DirectRLEnv):
         self._swing_peak[env_ids] = torch.tensor([0.0, 0.0, 0.0, 0.0], device=self.device)
         
         # Reset contact periodic
-        self._phase_signal[env_ids] = self._phase_offset[env_ids]# + self.step_dt * self._step_freq * torch.rand(env_ids.shape[0], 1, device=self.device)*10.
+        self._phase_signal[env_ids] = self._phase_offset[env_ids].clone()# + self.step_dt * self._step_freq * torch.rand(env_ids.shape[0], 1, device=self.device)*10.
         self._phase_signal[env_ids] = self._phase_signal[env_ids]  % 1.0
 
         # Reset robot state