--- git status ---
HEAD detached from dc9bbf2
Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   deploy/config.py
	modified:   source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/assets/hyqreal_asset.py
	modified:   source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py
	modified:   source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py
	deleted:    tested_policies/aliengo/8k_128_128_128_stop/model_7900.pt

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	tested_policies/aliengo/8k_128_128_128_aliengo_stop_and_go/

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/deploy/config.py b/deploy/config.py
index 88989ea..f5ed2f5 100644
--- a/deploy/config.py
+++ b/deploy/config.py
@@ -7,7 +7,7 @@ sys.path.append(dir_path+"/../scripts/rsl_rl")
 robot = 'aliengo'  # 'aliengo', 'go1', 'go2', 'b2', 'hyqreal1', 'hyqreal2', 'mini_cheetah'  # TODO: Load from robot_descriptions.py
 
 #policy_path = ".."
-policy_path = dir_path + "/../tested_policies/" + robot + "/8k_128_128_128_stop" + "/exported/policy.onnx"
+policy_path = "/home/alienware/isaaclab_ws_home/basic-locomotion-dls-isaaclab/logs/rsl_rl/rough_direct/2025-07-22_23-20-08_8k_128_128_128_aliengo/exported/policy.onnx"
 
 # ----------------------------------------------------------------------------------------------------------------
 if(robot == "aliengo"):
diff --git a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/assets/hyqreal_asset.py b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/assets/hyqreal_asset.py
index a379648..23fb04e 100644
--- a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/assets/hyqreal_asset.py
+++ b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/assets/hyqreal_asset.py
@@ -12,8 +12,8 @@ from basic_locomotion_dls_isaaclab.assets import ISAAC_ASSET_DIR
 
 
 # HYQREAL robot configuration from mujoco
-stiffness_mujoco = 200.0
-damping_mujoco = 20.0
+stiffness_mujoco = 175.0#200.0
+damping_mujoco = 14.0#20.0
 friction_static_mujoco = 0.2
 friction_dynamic_mujoco = 0.6
 armature_mujoco = 0.01
diff --git a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py
index 0da22b7..0722310 100644
--- a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py
+++ b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/agents/rsl_rl_ppo_cfg.py
@@ -45,7 +45,7 @@ class MorphologycalSymmetriesCfg:
 @configclass
 class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
     num_steps_per_env = 24
-    max_iterations = 500
+    max_iterations = 1500
     save_interval = 50
     experiment_name = "flat_direct"
     empirical_normalization = False
@@ -57,7 +57,7 @@ class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
         activation="elu",
     )
     algorithm = RslRlPpoAlgorithmCfg(
-        class_name="PPO", #PPO, PPOSymmDataAugmented #AMP_PPO
+        class_name="PPOSymmDataAugmented", #PPO, PPOSymmDataAugmented #AMP_PPO
         value_loss_coef=1.0,
         use_clipped_value_loss=True,
         clip_param=0.2,
@@ -86,8 +86,9 @@ class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
     )
 
     # Symmetry Related Stuff
+    history_length = 5
     morphologycal_symmetries_cfg = MorphologycalSymmetriesCfg(
-        obs_space_names = [
+        obs_space_names = ([
             "base_lin_vel:base",
             "base_ang_vel:base",
             "gravity:base",
@@ -96,7 +97,7 @@ class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
             "qvel_js",
             "actions",
             "clock_data",
-        ],
+        ])*int(history_length),
         
         action_space_names = ["actions"],
         
@@ -106,8 +107,6 @@ class FlatPPORunnerCfg(RslRlOnPolicyRunnerCfg):
             "FL_calf_joint", "FR_calf_joint", "RL_calf_joint", "RR_calf_joint"
         ],
         
-        history_length = 5,
-        
         robot_name = "a1",
     )
 
@@ -129,7 +128,7 @@ class RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
         activation="elu",
     )
     algorithm = RslRlPpoAlgorithmCfg(
-        class_name="PPO", #PPO, PPOSymmDataAugmented #AMP_PPO
+        class_name="PPOSymmDataAugmented", #PPO, PPOSymmDataAugmented #AMP_PPO
         value_loss_coef=1.0,
         use_clipped_value_loss=True,
         clip_param=0.2,
diff --git a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py
index e37d825..37c1164 100644
--- a/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py
+++ b/source/basic_locomotion_dls_isaaclab/basic_locomotion_dls_isaaclab/tasks/locomotion/locomotion_env.py
@@ -69,7 +69,7 @@ class LocomotionEnv(DirectRLEnv):
         elif(cfg.desired_gait == "multigait"):
             #TODO: implement multigait
             raise NotImplementedError("Multigait not implemented yet")
-        self._phase_signal = self._phase_offset# + self.step_dt * self._step_freq * torch.rand(self.num_envs, 1, device=self.device)*10.
+        self._phase_signal = self._phase_offset.clone()# + self.step_dt * self._step_freq * torch.rand(self.num_envs, 1, device=self.device)*10.
         self._phase_signal = self._phase_signal % 1.0
 
 
@@ -181,25 +181,28 @@ class LocomotionEnv(DirectRLEnv):
         self._commands[:, :3] = self._commands[:, :3] * ~resample_time.unsqueeze(1).expand(-1, 3) + commands_resample * resample_time.unsqueeze(1).expand(-1, 3)
 
 
-        # Stop and Go
+        # Stop
+        rest_time = self.episode_length_buf >= self.max_episode_length - 50
+        self._commands[:, :3] *= ~rest_time.unsqueeze(1).expand(-1, 3)        
+
+
+        """# Stop and Go
+        rest_time = (self.episode_length_buf >= self.max_episode_length - 150) & (self.episode_length_buf <= self.max_episode_length - 100)
+        self._commands[:, :3] *= ~rest_time.unsqueeze(1).expand(-1, 3)        
+
         restart_time = self.episode_length_buf == self.max_episode_length - 99
         commands_restart = torch.zeros_like(self._commands).uniform_(-1.0, 1.0)
         commands_restart[:, 0] *= 0.5 * self._velocity_gait_multiplier
         commands_restart[:, 1] *= 0.25 
         commands_restart[:, 2] *= 0.3 
-        self._commands[:, :3] = self._commands[:, :3] * ~restart_time.unsqueeze(1).expand(-1, 3) + commands_restart * restart_time.unsqueeze(1).expand(-1, 3)
-
-        rest_time = (self.episode_length_buf >= self.max_episode_length - 150) & (self.episode_length_buf <= self.max_episode_length - 100)
-        self._commands[:, :3] *= ~rest_time.unsqueeze(1).expand(-1, 3)
+        self._commands[:, :3] = self._commands[:, :3] * ~restart_time.unsqueeze(1).expand(-1, 3) + commands_restart * restart_time.unsqueeze(1).expand(-1, 3)"""
 
 
         # Took some envs, and put to zero the vel
         if self.num_envs > 100:
             num_fixed_envs = 100
             fixed_env_ids = torch.arange(num_fixed_envs, device=self.device)
-            self._commands[fixed_env_ids, 0] = 0.0
-            self._commands[fixed_env_ids, 1] = 0.0
-            self._commands[fixed_env_ids, 2] = 0.0
+            self._commands[fixed_env_ids, :3] *= 0.0
 
 
         clock_data = None
@@ -749,7 +752,7 @@ class LocomotionEnv(DirectRLEnv):
         self._swing_peak[env_ids] = torch.tensor([0.0, 0.0, 0.0, 0.0], device=self.device)
         
         # Reset contact periodic
-        self._phase_signal[env_ids] = self._phase_offset[env_ids]# + self.step_dt * self._step_freq * torch.rand(env_ids.shape[0], 1, device=self.device)*10.
+        self._phase_signal[env_ids] = self._phase_offset[env_ids].clone()# + self.step_dt * self._step_freq * torch.rand(env_ids.shape[0], 1, device=self.device)*10.
         self._phase_signal[env_ids] = self._phase_signal[env_ids]  % 1.0
 
         # Reset robot state
diff --git a/tested_policies/aliengo/8k_128_128_128_stop/model_7900.pt b/tested_policies/aliengo/8k_128_128_128_stop/model_7900.pt
deleted file mode 100644
index 8913ff5..0000000
--- a/tested_policies/aliengo/8k_128_128_128_stop/model_7900.pt
+++ /dev/null
@@ -1,3 +0,0 @@
-version https://git-lfs.github.com/spec/v1
-oid sha256:89654ebaa63bd5c5fce5ee4870e348b2244d42185f1e3946f8d7d74b3dc441d7
-size 1634722